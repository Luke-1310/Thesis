APPUNTI SUI DIVERSI FILE PRESENTI NEL PROGETTO, AGGIORNATO AL 28/07/2025

**q_learning_training.py:**

La Q-table è una tabella che usa un agente di Q-learning (tipologia di reinforcement learning) per imparare quale azione fare in ogni stato.
    Ogni riga rappresenta uno stato (ad esempio: "sono all'incrocio, il semaforo è rosso").
    Ogni colonna rappresenta una azione possibile (tipo "vai dritto", "gira a destra", "aspetta").
    Ogni valore dentro la tabella (Q-value) dice quanto è buona quell'azione in quello stato.

def train_agent(env, font): passati enviornment e font, tale funzione ha come scopo quello di addestrare l'agente con diversi parametri.

def evaluate_agent(env, font): si occupa di valutare l'agente, scegliendo tramite np.argmax... (173) l'azione migliore da compiere;
    chiaramente ci sono due esiti finali, il raggiungimento del parcheggio oppure il fallimento.

def show_menu(screen, font): passandogli il font e l'interfaccia dove disegnare (finestra) viene disegnato un menu grafico per l'utente

def draw_text(screen, text, x, y, font, color=(0,0,0), center=False): funzione per stampare testo su schermo

def show_yes_no_dialog(screen, font, question): mostra all'utente se vuole vedere o no i risultati

def select_map(screen, font): permette di scegliere la mappa all'utente e ritorna la classe della mappa selezionata

def show_training_results(screen, font, episode_data): stampa il resecondo del training dell'agente

def show_settings(screen, font, env): funzione attua nel gestire le impostazioni del simulatore

def show_training_charts(screen, font, episode_data, cumulative_collisions, env): per stampare i grafici con i risultati delle simulazioni

def main

**base_environment.py:**

def __init__(self, width, height, cell_size, screen = None, num_pedoni = 0, pedone_error_prob=0.0, route_change_probability=0, num_episodi=2000): si occupa di 
    inizializzare le variabili d'ambiente 

def load_assets(self): ricordando che self è una convenzione in Python che rappresenta l'istanza corrente di una classe, tale funzione carica gli le diverse risorse

def create_grid(self): crea la griglia dell'ambiente

def is_car_in_vision(self): verifica se una delle auto si trova nel campo visivo dell'agente

def update_car_position(self): aggiorna la posizione di una singola auto secondo il suo percorso

def check_and_change_route(self, car): qual ora un auto si dovesse trovare ad un incriocio, con una certà probabilità fornita dall'utente, glielo fa cambiare

def _calculate_rotation(self, car): Serve per far ruotare la macchina durante il training

def get_next_action(self, epsilon): Tale funzione fa scegliere all'agente, in base a dei parametri e sulle conoscenze pregresse, se deve espolrare o "sfruttare"

def is is_valid_move(self, new_position): Valutando la griglia di partenza, fa sì che l'agente non esca fuori dalla strada

def get_next_location(self, action_index): In base alla successiva posizione dell'agente, farà sì che l'auto ruoti con una certa angolatura

def check_goal(self): se l'agente arriva in una delle posizioni "goal" allora vince

def check_loss(self): controlla se l'agente collide con un auto nemica oppure con un pedone

def update_traffic_lights(self): aggiorna i diversi semafori presenti nella mappa

def display(self, episode=None, path=None): Funzione cardine nel disegnare a schermo tutto quello che vede l'utente durante il training

def _display_car(self, car_image, car_position, car_rotation): Richiamata nella funzione precedente, essa si occupa di ruotare le auto durante la simulazione

def pedone_path_callback(self, start, can_make_errors=True):

    Il pedone decide se sbagliare in diverse fasi: 
        All'inizio: Quando viene creato
        Durante il movimento: Quando arriva a destinazione e calcola un nuovo percorso
        Ma non ad ogni step: L'errore è nella destinazione, non nel movimento

def reset_game(self): riavvia il simulatore per un nuovo Episodio

def heuristic(self, a, b): essa calcola la distnaza tra due punti tramite la seguente formula:
    "DISTANZA DI MANHATTAN": distanza = |x1-x2| + |y1-y2| 
                             distanza = abs(2 - 5) + abs(3 - 7) = abs(-3) + abs(-4) = 3 + 4 = 7
                             abs è la funzione valore assoluto ed infatti sta per "absolute"
    Si chiama così perché ci si immagina di essere a New York con strade a griglia: NON POSSIAMO ANDARE IN DIAGONALE MA SOLO su, giù, sinistra, destra

def find_path(self, grid, start, goal, walkable_value=(1,2), cost_matrix=None): Si utilizza l'algoritmo A* per trovare il percorso più breve tra due punti in una griglia

    L’algoritmo A* funziona in modo simile a Dijkstra, ma introduce un’euristica che stima quanto manca per raggiungere l’obiettivo, rendendo l’esplorazione più mirata ed efficiente. 
    L’idea di fondo è che, se una destinazione è lontana, riceverà una priorità più bassa nella coda, poiché il costo stimato per raggiungerla sarà maggiore. A* ordina i nodi da esplorare 
    in base a una funzione f(n)=g(n)+h(n). dove g(n) rappresenta il costo effettivo dal punto di partenza al nodo corrente, e h(n) è una stima della distanza residua fino alla meta. 
    In questo modo, l’algoritmo dà priorità ai nodi che sembrano andare nella giusta direzione, senza trascurare il costo reale del percorso già compiuto. Tutti i nodi non ancora visitati 
    inizialmente hanno una distanza infinita, proprio come in Dijkstra, ma A* si distingue perché combina queste due misure per prendere decisioni più intelligenti. Durante l’esecuzione, 
    viene scelto il nodo con il valore di f(n) più basso, e da lì si espandono i nodi successivi. Un vantaggio importante di A* è che, se l’euristica usata è ammissibile (cioè non sovrastima mai la distanza reale), 
    allora non appena raggiunge l’obiettivo, possiamo essere certi che il percorso trovato è il più breve possibile. Questo non è vero per Dijkstra, che deve continuare a esplorare fino alla fine senza poter 
    "tagliare corto", perché non ha una guida che lo indirizzi verso l'obiettivo.

def move_pedone_along_path(self, pedone, path): sposta il pedone dalla posizione attuale a quella successiva

def update_pedoni(self, pedoni): per ciascun pedone, ne aggiorna la posizione

def check_collision_type(self): controlla con chi ha colliso l'agente

def _find_nearest_valid_cell(self, target): funzione per trovare la cella percorribile più vicina a una cella non percorribile

def _create_error_segment(self, valid_end, error_target): Crea un segmento di percorso che porta dall'ultima cella valida al target di errore 

**map1_environment.py:** ESTENDE BaseEnvironment

def __init__(self, width, height, cell_size, screen = None, num_pedoni = 0, pedone_error_prob=0.0, route_change_probability=0, num_episodi=2000): si occupa di 
    inizializzare le variabili d'ambiente ed aggiunge informazioni specifiche della mappa corrente come il nome e la posizione di inizio dell'agente

def load_assets(self): ricordando che self è una convenzione in Python che rappresenta l'istanza corrente di una classe, tale funzione carica gli le diverse risorse

def create_grid(self): creo la griglia con tutti gli elementi, come, i percorsi delle auto nemiche, i semafori, gli incroci, la mappa per i pedoni, 
                       inizializza la matrice dei costi per i pedoni,  matrice dei reward e penalità

**map2_environment.py:**

similare a map1_environment

**pedone.py**

def __init__(self, start, goal, path=None, wait_steps = 5, path_callback=None, error_prob=0.0): inizializza tutto il necessario per i pedoni, come il punto di partenza, il goal,
                                                                                                il percorso e la probabilità che esso ha di sbagliare
def step(self, map_pedone, traffic_lights=None): fa avanzare il pedone lungo il percorso, gestendo diversi casi tra cui se arriva a destinazione                                                                          

----------PROBLEMI------------

ho notato che durante la q table foresta un'auto nemica passa sopra l'auto agente, strano

08/08/2025 mi sono accorto di un bel problema, i pedoni non sono presenti nella q table quindi suppongo che l'agente non impara veramente, errore logico importante, da sistemare!
13/08/2025 ora sono stati aggiunti
16/08/2025 dovrebbe andare bene ora, prossimo passo avere + q table e poterle selezionare
17/08/2025 per risolvere il problema della sovrapposizione delle auto e dei pedoni a tempo di visualizzazione si potrebbero levare oppure cercare di gestire il problema a tempo di inferenza

0) questioniaro almalaurea
1) risolvere il problema dell'inferenza durante la visualizzazione della q table
2) scrivere parte della tesi sul reinforcement learning e su A*

3) eventuali colloqui con chat gpt + ripassare la parte tecnica

30/08/2025 provo ad introdurre anche i semafori nel training dell'agente (riprendere dal punto 3)
31/08/2025 if per semafori
1/09/2025 ho pensato di introdurre i semafori con un if, in modo tale che da avere due versioni del simulatore, una complicata e una semplificata dove quella complicata tende a seguire le regole della strafa

DOMANDE PER RICEVIMENTO 09/09

Noto che impostando un il campo di visivo per i pedoni 2x2 l'agente fa attenzione anche ai pedoni sulla carreggiata